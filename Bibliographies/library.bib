Automatically generated by Mendeley Desktop 1.17.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Excercise2012,
author = {Excercise, T},
file = {:home/morispi1/Cours/PhD/Articles/Moi/template{\_}jobim2017/Proceedings/LaTeX/jobim{\_}proceedings.pdf:pdf},
keywords = {11-point font,five relevant keywords maximum,important,margins must,must be typeset in,regular,relevant,some keywords,the keywords,the left and right,with times new roman},
number = {2},
pages = {2--3},
title = {{Template for}},
year = {2012}
}
@article{Marcais2015,
abstract = {Motivation Illumina Sequencing data can provide high coverage of a genome by relatively short (most often 100 bp to 150 bp) reads at a low cost. Even with low (advertised 1{\%}) error rate, 100 × coverage Illumina data on average has an error in some read at every base in the genome. These errors make handling the data more complicated because they result in a large number of low-count erroneous k-mers in the reads. However, there is enough information in the reads to correct most of the sequencing errors, thus making subsequent use of the data (e.g. for mapping or assembly) easier. Here we use the term “error correction” to denote the reduction in errors due to both changes in individual bases and trimming of unusable sequence. We developed an error correction software called QuorUM. QuorUM is mainly aimed at error correcting Illumina reads for subsequent assembly. It is designed around the novel idea of minimizing the number of distinct erroneous k-mers in the output reads and preserving the most true k-mers, and we introduce a composite statistic $\pi$ that measures how successful we are at achieving this dual goal. We evaluate the performance of QuorUM by correcting actual Illumina reads from genomes for which a reference assembly is available. Results We produce trimmed and error-corrected reads that result in assemblies with longer contigs and fewer errors. We compared QuorUM against several published error correctors and found that it is the best performer in most metrics we use. QuorUM is efficiently implemented making use of current multi-core computing architectures and it is suitable for large data sets (1 billion bases checked and corrected per day per core). We also demonstrate that a third-party assembler (SOAPdenovo) benefits significantly from using QuorUM error-corrected reads. QuorUM error corrected reads result in a factor of 1.1 to 4 improvement in N50 contig size compared to using the original reads with SOAPdenovo for the data sets investigated. Availability QuorUM is distributed as an independent software package and as a module of the MaSuRCA assembly software. Both are available under the GPL open source license at http://www.genome.umd.edu. Contact gmarcais@umd.edu.},
author = {Mar{\c{c}}ais, Guillaume and Yorke, James A and Zimin, Aleksey},
doi = {10.1371/journal.pone.0130821},
file = {:home/morispi1/Cours/PhD/Articles/Correction/QuorUM.pdf:pdf},
journal = {PLOS ONE},
number = {6},
pages = {1--13},
publisher = {Public Library of Science},
title = {{QuorUM: An Error Corrector for Illumina Reads}},
url = {http://dx.doi.org/10.1371{\%}2Fjournal.pone.0130821},
volume = {10},
year = {2015}
}
@article{Chaisson2012,
abstract = {BACKGROUND: Recent methods have been developed to perform high-throughput sequencing of DNA by Single Molecule Sequencing (SMS). While Next-Generation sequencing methods may produce reads up to several hundred bases long, SMS sequencing produces reads up to tens of kilobases long. Existing alignment methods are either too inefficient for high-throughput datasets, or not sensitive enough to align SMS reads, which have a higher error rate than Next-Generation sequencing.$\backslash$n$\backslash$nRESULTS: We describe the method BLASR (Basic Local Alignment with Successive Refinement) for mapping Single Molecule Sequencing (SMS) reads that are thousands of bases long, with divergence between the read and genome dominated by insertion and deletion error. The method is benchmarked using both simulated reads and reads from a bacterial sequencing project. We also present a combinatorial model of sequencing error that motivates why our approach is effective.$\backslash$n$\backslash$nCONCLUSIONS: The results indicate that it is possible to map SMS reads with high accuracy and speed. Furthermore, the inferences made on the mapability of SMS reads using our combinatorial model of sequencing error are in agreement with the mapping accuracy demonstrated on simulated reads.},
author = {Chaisson, Mark J and Tesler, Glenn},
doi = {10.1186/1471-2105-13-238},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaisson, Tesler - 2012 - Mapping single molecule sequencing reads using basic local alignment with successive refinement (BLASR) applic.pdf:pdf},
isbn = {1471-2105 (Electronic)$\backslash$n1471-2105 (Linking)},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Base Sequence,DNA,DNA: genetics,DNA: methods,Genome,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis,Software},
number = {1},
pages = {238},
pmid = {22988817},
title = {{Mapping single molecule sequencing reads using basic local alignment with successive refinement (BLASR): application and theory.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3572422{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {13},
year = {2012}
}
@article{Birol2015,
abstract = {De novo assembly of the genome of a species is essential in the absence of a reference genome sequence. Many scalable assembly algorithms use the de Bruijn graph (DBG) paradigm to reconstruct genomes, where a table of subsequences of a certain length is derived from the reads, and their overlaps are analyzed to assemble sequences. Despite longer subsequences unlocking longer genomic features for assembly, associated increase in compute resources limits the practicability of DBG over other assembly archetypes already designed for longer reads. Here, we revisit the DBG paradigm to adapt it to the changing sequencing technology landscape and introduce three data structure designs for spaced seeds in the form of paired subsequences. These data structures address memory and run time constraints imposed by longer reads. We observe that when a fixed distance separates seed pairs, it provides increased sequence specificity with increased gap length. Further, we note that Bloom filters would be suitable to implicitly store spaced seeds and be tolerant to sequencing errors. Building on this concept, we describe a data structure for tracking the frequencies of observed spaced seeds. These data structure designs will have applications in genome, transcriptome and metagenome assemblies, and read error correction.},
author = {Birol, Inan{\c{c}} and Chu, Justin and Mohamadi, Hamid and Jackman, Shaun D and Raghavan, Karthika and Vandervalk, Benjamin P and Raymond, Anthony and Warren, Ren{\'{e}} L},
doi = {10.1155/2015/196591},
file = {:home/morispi1/Cours/PhD/Articles/Divers/Spaced{\_}seeds{\_}data{\_}structures{\_}de{\_}novo{\_}assembly.pdf:pdf},
issn = {23144378},
journal = {International Journal of Genomics},
pmid = {26539459},
title = {{Spaced Seed Data Structures for De Novo Assembly}},
volume = {2015},
year = {2015}
}
@article{Hackl2014,
abstract = {MOTIVATION:: Today, the base code of DNA is mostly determined through sequencing by synthesis as provided by the Illumina sequencers. Although highly accurate, resulting reads are short, making their analyses challenging. Recently, a new technology, single molecule real-time (SMRT) sequencing, was developed that could address these challenges, as it generates reads of several thousand bases. But, their broad application has been hampered by a high error rate. Therefore, hybrid approaches that use high-quality short reads to correct erroneous SMRT long reads have been developed. Still, current implementations have great demands on hardware, work only in well-defined computing infrastructures and reject a substantial amount of reads. This limits their usability considerably, especially in the case of large sequencing projects.$\backslash$n$\backslash$nRESULTS:: Here we present proovread, a hybrid correction pipeline for SMRT reads, which can be flexibly adapted on existing hardware and infrastructure from a laptop to a high-performance computing cluster. On genomic and transcriptomic test cases covering Escherichia coli, Arabidopsis thaliana and human, proovread achieved accuracies up to 99.9{\%} and outperformed the existing hybrid correction programs. Furthermore, proovread-corrected sequences were longer and the throughput was higher. Thus, proovread combines the most accurate correction results with an excellent adaptability to the available hardware. It will therefore increase the applicability and value of SMRT sequencing. Availability and implementation: proovread is available at the following URL: http://proovread.bioapps.biozentrum.uni-wuerzburg.de CONTACT: : frank.foerster@biozentrum.uni-wuerzburg.de Supplementary information: Supplementary data are available at Bioinformatics online.},
author = {Hackl, Thomas and Hedrich, Rainer and Schultz, J{\"{o}}rg and F{\"{o}}rster, Frank},
doi = {10.1093/bioinformatics/btu392},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hackl et al. - 2014 - Proovread Large-scale high-accuracy PacBio correction through iterative short read consensus.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {14602059},
journal = {Bioinformatics},
number = {21},
pages = {3004--3011},
pmid = {25015988},
title = {{Proovread: Large-scale high-accuracy PacBio correction through iterative short read consensus}},
volume = {30},
year = {2014}
}
@article{Liu2012,
abstract = {SOAP3 is the first short read alignment tool that leverages the multi-processors in a graphic processing unit (GPU) to achieve a drastic improvement in speed. We adapted the compressed full-text index (BWT) used by SOAP2 in view of the advantages and disadvantages of GPU. When tested with millions of Illumina Hiseq 2000 length-100 bp reads, SOAP3 takes {\textless} 30 s to align a million read pairs onto the human reference genome and is at least 7.5 and 20 times faster than BWA and Bowtie, respectively. For aligning reads with up to four mismatches, SOAP3 aligns slightly more reads than BWA and Bowtie; this is because SOAP3, unlike BWA and Bowtie, is not heuristic-based and always reports all answers.},
author = {Liu, Chi Man and Wong, Thomas and Wu, Edward and Luo, Ruibang and Yiu, Siu Ming and Li, Yingrui and Wang, Bingqiang and Yu, Chang and Chu, Xiaowen and Zhao, Kaiyong and Li, Ruiqiang and Lam, Tak Wah},
doi = {10.1093/bioinformatics/bts061},
file = {:home/morispi1/Cours/PhD/Articles/Alignement/SOAP3.pdf:pdf},
isbn = {1367480314602059},
issn = {13674803},
journal = {Bioinformatics},
number = {6},
pages = {878--879},
pmid = {22285832},
title = {{SOAP3: Ultra-fast GPU-based parallel alignment tool for short reads}},
volume = {28},
year = {2012}
}
@article{deBruijn1946,
author = {de Bruijn, Nicolaas Govert},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/N.G. de Bruijn - 1946 - A combinatorial problem.pdf:pdf},
journal = {Koninklijke Nederlandse Akademie van Wetenschappen te Amsterdam},
number = {7},
pages = {758--764},
title = {{A combinatorial problem}},
volume = {49},
year = {1946}
}
@article{Corresponding,
author = {Corresponding, Country},
file = {:home/morispi1/Cours/PhD/Articles/Moi/template{\_}jobim2017/Proceedings/LaTeX/HG-CoLoR.pdf:pdf},
keywords = {assembly,correction,long reads,ngs},
pages = {1--8},
title = {{HG-CoLoR: A new method for the production of synthetic long reads}}
}
@article{Li2009a,
abstract = {MOTIVATION: The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals.$\backslash$n$\backslash$nRESULTS: We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows-Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is approximately 10-20x faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package.$\backslash$n$\backslash$nAVAILABILITY: http://maq.sourceforge.net.},
author = {Li, Heng and Durbin, Richard},
doi = {10.1093/bioinformatics/btp324},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Durbin - 2009 - Fast and accurate short read alignment with Burrows-Wheeler transform.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {14},
pages = {1754--1760},
pmid = {19451168},
title = {{Fast and accurate short read alignment with Burrows-Wheeler transform}},
volume = {25},
year = {2009}
}
@article{Holley2015,
abstract = {We present a new algorithm for unbound (real life) docking of molecules, whether protein-protein or protein-drug. The algorithm carries out rigid docking, with surface variability/flexibility implicitly addressed through liberal intermolecular penetration. The high efficiency of the algorithm is the outcome of several factors: (i) focusing initial molecular surface fitting on localized, curvature based surface patches; (ii) use of Geometric Hashing and Pose Clustering for initial transformation detection; (iii) accurate computation of shape complementarity utilizing the Distance Transform; (iv) efficient steric clash detection and geometric fit scoring based on a multi-resolution shape representation; and (v) utilization of biological information by focusing on hot spot rich surface patches. The algorithm has been implemented and applied to a large number of cases.},
archivePrefix = {arXiv},
arxivId = {1010.2656},
author = {Holley, Guillaume and Wittler, Roland and Stoye, Jens},
doi = {10.1007/978-3-662-48221-6_16},
eprint = {1010.2656},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Holley, Wittler, Stoye - 2015 - Bloom Filter Trie – A Data Structure for Pan-Genome Storage.pdf:pdf},
isbn = {9783662482209},
issn = {16113349},
journal = {Algorithms in Bioinformatics: 15th International Workshop, WABI 2015},
pages = {217--230},
title = {{Bloom Filter Trie – A Data Structure for Pan-Genome Storage}},
url = {http://link.springer.com/10.1007/978-3-662-48221-6{\_}16},
year = {2015}
}
@article{Tange2011,
author = {Tange, Ole},
file = {:home/morispi1/Cours/PhD/Articles/Divers/Parallel.pdf:pdf},
journal = {;login: The USENIX Magazine},
number = {1},
pages = {42--47},
title = {{GNU Parallel - The Command-Line Power Tool}},
volume = {36},
year = {2011}
}
@article{Ilie2011,
abstract = {High-throughput sequencing technologies produce very large amounts of data and sequencing errors constitute one of the major problems in analyzing such data. Current algorithms for correcting these errors are not very accurate and do not automatically adapt to the given data.},
author = {Ilie, Lucian and Fazayeli, Farideh and Ilie, Silvana},
doi = {10.1093/bioinformatics/btq653},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ilie, Fazayeli, Ilie - 2011 - HiTEC Accurate error correction in high-throughput sequencing data.pdf:pdf},
isbn = {1367-4803},
issn = {13674803},
journal = {Bioinformatics},
number = {3},
pages = {295--302},
pmid = {21115437},
title = {{HiTEC: Accurate error correction in high-throughput sequencing data}},
volume = {27},
year = {2011}
}
@article{Mardis2008,
abstract = {If one accepts that the fundamental pursuit of genetics is to determine the genotypes that explain phenotypes, the meteoric increase of DNA sequence information applied toward that pursuit has nowhere to go but up. The recent introduction of instruments capable of producing millions of DNA sequence reads in a single run is rapidly changing the landscape of genetics, providing the ability to answer questions with heretofore unimaginable speed. These technologies will provide an inexpensive, genome-wide sequence readout as an endpoint to applications ranging from chromatin immunoprecipitation, mutation mapping and polymorphism discovery to noncoding RNA discovery. Here I survey next-generation sequencing technologies and consider how they can provide a more complete picture of how the genome shapes the organism. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Mardis, Elaine R.},
doi = {10.1016/j.tig.2007.12.007},
eprint = {NIHMS150003},
file = {:home/morispi1/Cours/PhD/Articles/Divers/Mardis.pdf:pdf},
isbn = {0168-9525 (Print)$\backslash$r0168-9525 (Linking)},
issn = {01689525},
journal = {Trends in Genetics},
number = {3},
pages = {133--141},
pmid = {18262675},
title = {{The impact of next-generation sequencing technology on genetics}},
volume = {24},
year = {2008}
}
@article{Larkin2007,
abstract = {We study a family of "classical" orthogonal polynomials which satisfy (apart from a 3-term recurrence relation) an eigenvalue problem with a differential operator of Dunkl-type. These polynomials can be obtained from the little {\$}q{\$}-Jacobi polynomials in the limit {\$}q=-1{\$}. We also show that these polynomials provide a nontrivial realization of the Askey-Wilson algebra for {\$}q=-1{\$}.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Larkin, M. A. and Blackshields, G. and Brown, N. P. and Chenna, R. and Mcgettigan, P. A. and McWilliam, H. and Valentin, F. and Wallace, I. M. and Wilm, A. and Lopez, R. and Thompson, J. D. and Gibson, T. J. and Higgins, D. G.},
doi = {10.1093/bioinformatics/btm404},
eprint = {1011.1669},
file = {:home/morispi1/Cours/PhD/Articles/Divers/clustalw2.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {21},
pages = {2947--2948},
pmid = {17846036},
title = {{Clustal W and Clustal X version 2.0}},
volume = {23},
year = {2007}
}
@article{Kelley2010,
abstract = {We introduce Quake, a program to detect and correct errors in DNA sequencing reads. Using a maximum likelihood approach incorporating quality values and nucleotide specific miscall rates, Quake achieves the highest accuracy on realistically simulated reads. We further demonstrate substantial improvements in de novo assembly and SNP detection after using Quake. Quake can be used for any size project, including more than one billion human reads, and is freely available as open source software from http://www.cbcb.umd.edu/software/quake.},
author = {Kelley, David R and Schatz, Michael C and Salzberg, Steven L},
doi = {10.1186/gb-2010-11-11-r116},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kelley, Schatz, Salzberg - 2010 - Quake quality-aware detection and correction of sequencing errors.pdf:pdf},
isbn = {1465-6906},
issn = {1465-6906},
journal = {Genome biology},
number = {11},
pages = {R116},
pmid = {21114842},
title = {{Quake: quality-aware detection and correction of sequencing errors.}},
url = {http://genomebiology.com/2010/11/11/R116},
volume = {11},
year = {2010}
}
@inbook{Ehrenfecht2004,
address = {Berlin, Heidelberg},
author = {Ehrenfeucht, Andrzej and Harju, Tero and Petre, Ion and Prescott, David M and Rozenberg, Grzegorz},
booktitle = {Computation in Living Cells: Gene Assembly in Ciliates},
doi = {10.1007/978-3-662-06371-2_10},
isbn = {978-3-662-06371-2},
pages = {99--108},
publisher = {Springer Berlin Heidelberg},
title = {{Overlap Graphs}},
url = {http://dx.doi.org/10.1007/978-3-662-06371-2{\_}10},
year = {2004}
}
@article{Salikhov2013,
abstract = {BACKGROUND: De Brujin graphs are widely used in bioinformatics for processing next-generation sequencing data. Due to a very large size of NGS datasets, it is essential to represent de Bruijn graphs compactly, and several approaches to this problem have been proposed recently.$\backslash$n$\backslash$nRESULTS: In this work, we show how to reduce the memory required by the data structure of Chikhi and Rizk (WABI'12) that represents de Brujin graphs using Bloom filters. Our method requires 30{\%} to 40{\%} less memory with respect to their method, with insignificant impact on construction time. At the same time, our experiments showed a better query time compared to the method of Chikhi and Rizk.$\backslash$n$\backslash$nCONCLUSION: The proposed data structure constitutes, to our knowledge, currently the most efficient practical representation of de Bruijn graphs.},
archivePrefix = {arXiv},
arxivId = {1302.7278},
author = {Salikhov, Kamil and Sacomoto, Gustavo and Kucherov, Gregory},
doi = {10.1007/978-3-642-40453-5_28},
eprint = {1302.7278},
file = {:home/morispi1/Cours/PhD/Articles/Assemblage/Minia (2013).pdf:pdf},
isbn = {9783642404528},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {364--376},
pmid = {24565280},
title = {{Using cascading bloom filters to improve the memory usage for de Brujin graphs}},
volume = {8126 LNBI},
year = {2013}
}
@article{Haghshenas2016,
abstract = {Motivation: Second generation sequencing technologies paved the way to an exceptional increase in the number of sequenced genomes, both prokaryotic and eukaryotic. However, short reads are difficult to assemble and often lead to highly fragmented assemblies. The recent developments in long reads sequencing methods offer a promising way to address this issue. However, so far long reads are characterized by a high error rate, and assembling from long reads require a high depth of coverage. This motivates the development of hybrid approaches that leverage the high quality of short reads to correct errors in long reads. Results: We introduce CoLoRMap, a hybrid method for correcting noisy long reads, such as the ones produced by PacBio sequencing technology, using high-quality Illumina paired-end reads mapped onto the long reads. Our algorithm is based on two novel ideas: using a classical shortest path algorithm to find a sequence of overlapping short reads that minimizes the edit score to a long read and extending corrected regions by local assembly of unmapped mates of mapped short reads. Our results on bacterial, fungal and insect data sets show that CoLoRMap compares well with existing hybrid correction methods. Availability and Implementation: The source code of CoLoRMap is freely available for non-commercial use at https:},
author = {Haghshenas, Ehsan and Hach, Faraz and Sahinalp, S Cenk and Chauve, Cedric},
doi = {10.1093/bioinformatics/btw463},
file = {:home/morispi1/Cours/PhD/Articles/Correction/Colormap.pdf:pdf},
isbn = {1367-4811 (Electronic)1367-4803 (Linking)},
issn = {1367-4803},
journal = {Bioinformatics},
number = {17},
pages = {i545--i551},
pmid = {27587673},
title = {{CoLoRMap: Correcting Long Reads by Mapping short reads}},
volume = {32},
year = {2016}
}
@inproceedings{Maillet2014,
abstract = {Metagenomics offers a way to analyze biotopes at the genomic level and to reach functional and taxonomical conclusions. The bio-analyzes of large metagenomic projects face critical limitations: complex metagenomes cannot be assembled and the taxonomical or functional annotations are much smaller than the real biological diversity. This motivated the development of de novo metagenomic read comparison approaches to extract information contained in metagenomic datasets. However, these new approaches do not scale up large metagenomic projects, or generate an important number of large intermediate and result files. We introduce Commet ('COmpare Multiple METagenomes'), a method that provides similarity overview between all datasets of large metagenomic projects. Directly from non-assembled reads, all against all comparisons are performed through an efficient indexing strategy. Then, results are stored as bit vectors, a compressed representation of read files, that can be used to further combine read subsets by common logical operations. Finally, Commet computes a clusterization of metagenomic datasets, which is visualized by dendrogram and heatmaps. Availability: http://github.com/pierrepeterlongo/commet},
archivePrefix = {arXiv},
arxivId = {1511.08317},
author = {Maillet, Nicolas and Collet, Guillaume and Vannier, Thomas and Lavenier, Dominique and Peterlongo, Pierre},
booktitle = {IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
doi = {10.1109/BIBM.2014.6999135},
eprint = {1511.08317},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maillet et al. - 2014 - Commet Comparing and combining multiple metagenomic datasets.pdf:pdf},
isbn = {9781479956692},
title = {{Commet: Comparing and combining multiple metagenomic datasets}},
year = {2014}
}
@article{Roberts2004,
abstract = {MOTIVATION: Comparison of nucleic acid and protein sequences is a fundamental tool of modern bioinformatics. A dominant method of such string matching is the 'seed-and-extend' approach, in which occurrences of short subsequences called 'seeds' are used to search for potentially longer matches in a large database of sequences. Each such potential match is then checked to see if it extends beyond the seed. To be effective, the seed-and-extend approach needs to catalogue seeds from virtually every substring in the database of search strings. Projects such as mammalian genome assemblies and large-scale protein matching, however, have such large sequence databases that the resulting list of seeds cannot be stored in RAM on a single computer. This significantly slows the matching process. RESULTS: We present a simple and elegant method in which only a small fraction of seeds, called 'minimizers', needs to be stored. Using minimizers can speed up string-matching computations by a large factor while missing only a small fraction of the matches found using all seeds.},
author = {Roberts, Michael and Hayes, Wayne and Hunt, Brian R. and Mount, Stephen M. and Yorke, James A.},
doi = {10.1093/bioinformatics/bth408},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Roberts et al. - 2004 - Reducing storage requirements for biological sequence comparison.pdf:pdf},
isbn = {1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {18},
pages = {3363--3369},
pmid = {15256412},
title = {{Reducing storage requirements for biological sequence comparison}},
volume = {20},
year = {2004}
}
@article{Li2010b,
abstract = {MOTIVATION: Many programs for aligning short sequencing reads to a reference genome have been developed in the last 2 years. Most of them are very efficient for short reads but inefficient or not applicable for reads {\textgreater}200 bp because the algorithms are heavily and specifically tuned for short queries with low sequencing error rate. However, some sequencing platforms already produce longer reads and others are expected to become available soon. For longer reads, hashing-based software such as BLAT and SSAHA2 remain the only choices. Nonetheless, these methods are substantially slower than short-read aligners in terms of aligned bases per unit time. RESULTS: We designed and implemented a new algorithm, Burrows-Wheeler Aligner's Smith-Waterman Alignment (BWA-SW), to align long sequences up to 1 Mb against a large sequence database (e.g. the human genome) with a few gigabytes of memory. The algorithm is as accurate as SSAHA2, more accurate than BLAT, and is several to tens of times faster than both. AVAILABILITY: http://bio-bwa.sourceforge.net},
archivePrefix = {arXiv},
arxivId = {1303.3997},
author = {Li, Heng and Durbin, Richard},
doi = {10.1093/bioinformatics/btp698},
eprint = {1303.3997},
file = {:home/morispi1/Cours/PhD/Articles/Alignement/BWA-SW.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$n1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {5},
pages = {589--595},
pmid = {20080505},
title = {{Fast and accurate long-read alignment with Burrows-Wheeler transform}},
volume = {26},
year = {2010}
}
@article{Hach2014,
abstract = {High throughput sequencing (HTS) platforms generate unprecedented amounts of data that introduce challenges for processing and downstream analysis. While tools that report the 'best' mapping location of each read provide a fast way to process HTS data, they are not suitable for many types of downstream analysis such as structural variation detection, where it is important to report multiple mapping loci for each read. For this purpose we introduce mrsFAST-Ultra, a fast, cache oblivious, SNP-aware aligner that can handle the multi-mapping of HTS reads very efficiently. mrsFAST-Ultra improves mrsFAST, our first cache oblivious read aligner capable of handling multi-mapping reads, through new and compact index structures that reduce not only the overall memory usage but also the number of CPU operations per alignment. In fact the size of the index generated by mrsFAST-Ultra is 10 times smaller than that of mrsFAST. As importantly, mrsFAST-Ultra introduces new features such as being able to (i) obtain the best mapping loci for each read, and (ii) return all reads that have at most n mapping loci (within an error threshold), together with these loci, for any user specified n. Furthermore, mrsFAST-Ultra is SNP-aware, i.e. it can map reads to reference genome while discounting the mismatches that occur at common SNP locations provided by db-SNP; this significantly increases the number of reads that can be mapped to the reference genome. Notice that all of the above features are implemented within the index structure and are not simple post-processing steps and thus are performed highly efficiently. Finally, mrsFAST-Ultra utilizes multiple available cores and processors and can be tuned for various memory settings. Our results show that mrsFAST-Ultra is roughly five times faster than its predecessor mrsFAST. In comparison to newly enhanced popular tools such as Bowtie2, it is more sensitive (it can report 10 times or more mappings per read) and much faster (six times or more) in the multi-mapping mode. Furthermore, mrsFAST-Ultra has an index size of 2GB for the entire human reference genome, which is roughly half of that of Bowtie2. mrsFAST-Ultra is open source and it can be accessed at http://mrsfast.sourceforge.net.},
author = {Hach, Faraz and Sarrafi, Iman and Hormozdiari, Farhad and Alkan, Can and Eichler, Evan E and Sahinalp, S Cenk},
doi = {10.1093/nar/gku370},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hach et al. - 2014 - MrsFAST-Ultra A compact, SNP-aware mapper for high performance sequencing applications.pdf:pdf},
issn = {13624962},
journal = {Nucleic Acids Research},
number = {W1},
pages = {494--500},
pmid = {24810850},
title = {{MrsFAST-Ultra: A compact, SNP-aware mapper for high performance sequencing applications}},
volume = {42},
year = {2014}
}
@article{Marcais2011,
abstract = {MOTIVATION: Counting the number of occurrences of every k-mer (substring of length k) in a long string is a central subproblem in many applications, including genome assembly, error correction of sequencing reads, fast multiple sequence alignment and repeat detection. Recently, the deep sequence coverage generated by next-generation sequencing technologies has caused the amount of sequence to be processed during a genome project to grow rapidly, and has rendered current k-mer counting tools too slow and memory intensive. At the same time, large multicore computers have become commonplace in research facilities allowing for a new parallel computational paradigm.{\$}\backslash{\$}n{\$}\backslash{\$}nRESULTS: We propose a new k-mer counting algorithm and associated implementation, called Jellyfish, which is fast and memory efficient. It is based on a multithreaded, lock-free hash table optimized for counting k-mers up to 31 bases in length. Due to their flexibility, suffix arrays have been the data structure of choice for solving many string problems. For the task of k-mer counting, important in many biological applications, Jellyfish offers a much faster and more memory-efficient solution.{\$}\backslash{\$}n{\$}\backslash{\$}nAVAILABILITY: The Jellyfish software is written in C++ and is GPL licensed. It is available for download at http://www.cbcb.umd.edu/software/jellyfish.},
author = {Mar{\c{c}}ais, Guillaume and Kingsford, Carl},
doi = {10.1093/bioinformatics/btr011},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mar{\c{c}}ais, Kingsford - 2011 - A fast, lock-free approach for efficient parallel counting of occurrences of k-mers.pdf:pdf},
isbn = {1367-4811 (Electronic){\$}\backslash{\$}n1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {6},
pages = {764--770},
pmid = {21217122},
title = {{A fast, lock-free approach for efficient parallel counting of occurrences of k-mers}},
volume = {27},
year = {2011}
}
@article{Miclotte2016,
abstract = {BACKGROUND: Third generation sequencing platforms produce longer reads with higher error rates than second generation technologies. While the improved read length can provide useful information for downstream analysis, underlying algorithms are challenged by the high error rate. Error correction methods in which accurate short reads are used to correct noisy long reads appear to be attractive to generate high-quality long reads. Methods that align short reads to long reads do not optimally use the information contained in the second generation data, and suffer from large runtimes. Recently, a new hybrid error correcting method has been proposed, where the second generation data is first assembled into a de Bruijn graph, on which the long reads are then aligned. RESULTS: In this context we present Jabba, a hybrid method to correct long third generation reads by mapping them on a corrected de Bruijn graph that was constructed from second generation data. Unique to our method is the use of a pseudo alignment approach with a seed-and-extend methodology, using maximal exact matches (MEMs) as seeds. In addition to benchmark results, certain theoretical results concerning the possibilities and limitations of the use of MEMs in the context of third generation reads are presented. CONCLUSION: Jabba produces highly reliable corrected reads: almost all corrected reads align to the reference, and these alignments have a very high identity. Many of the aligned reads are error-free. Additionally, Jabba corrects reads using a very low amount of CPU time. From this we conclude that pseudo alignment with MEMs is a fast and reliable method to map long highly erroneous sequences on a de Bruijn graph.},
author = {Miclotte, Giles and Heydari, Mahdi and Demeester, Piet and Rombauts, Stephane and {Van de Peer}, Yves and Audenaert, Pieter and Fostier, Jan},
doi = {10.1186/s13015-016-0075-7},
file = {:home/morispi1/Cours/PhD/Articles/Correction/Jabba.pdf:pdf},
isbn = {1748-7188 (Electronic) 1748-7188 (Linking)},
issn = {1748-7188},
journal = {Algorithms Mol Biol},
keywords = {Error correction,Maximal exact matches,Sequence analysis,de Bruijn graph},
pages = {10},
pmid = {27148393},
publisher = {BioMed Central},
title = {{Jabba: hybrid error correction for long sequencing reads}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27148393{\%}0Ahttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC4855726/pdf/13015{\_}2016{\_}Article{\_}75.pdf},
volume = {11},
year = {2016}
}
@article{Salmela2014,
abstract = {MOTIVATION: PacBio single molecule real-time sequencing is a third-generation sequencing technique producing long reads, with comparatively lower throughput and higher error rate. Errors include numerous indels and complicate downstream analysis like mapping or de novo assembly. A hybrid strategy that takes advantage of the high accuracy of second-generation short reads has been proposed for correcting long reads. Mapping of short reads on long reads provides sufficient coverage to eliminate up to 99{\%} of errors, however, at the expense of prohibitive running times and considerable amounts of disk and memory space. RESULTS: We present LoRDEC, a hybrid error correction method that builds a succinct de Bruijn graph representing the short reads, and seeks a corrective sequence for each erroneous region in the long reads by traversing chosen paths in the graph. In comparison, LoRDEC is at least six times faster and requires at least 93{\%} less memory or disk space than available tools, while achieving comparable accuracy. Availability and implementaion: LoRDEC is written in C++, tested on Linux platforms and freely available at http://atgc.lirmm.fr/lordec.},
author = {Salmela, Leena and Rivals, Eric},
doi = {10.1093/bioinformatics/btu538},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salmela, Rivals - 2014 - LoRDEC Accurate and efficient long read error correction.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {14602059},
journal = {Bioinformatics},
number = {24},
pages = {3506--3514},
pmid = {25165095},
title = {{LoRDEC: Accurate and efficient long read error correction}},
volume = {30},
year = {2014}
}
@article{Li2010a,
abstract = {Next-generation massively parallel DNA sequencing technologies provide ultrahigh throughput at a substantially lower unit data cost; however, the data are very short read length sequences, making de novo assembly extremely challenging. Here, we describe a novel method for de novo assembly of large genomes from short read sequences. We successfully assembled both the Asian and African human genome sequences, achieving an N50 contig size of 7.4 and 5.9 kilobases (kb) and scaffold of 446.3 and 61.9 kb, respectively. The development of this de novo short read assembly method creates new opportunities for building reference sequences and carrying out accurate analyses of unexplored genomes in a cost-effective way.},
author = {Li, Ruiqiang and Zhu, Hongmei and Ruan, Jue and Qian, Wubin and Fang, Xiaodong and Shi, Zhongbin and Li, Yingrui and Li, Shengting and Shan, Gao and Kristiansen, Karsten and Li, Songgang and Yang, Huanming and Wang, Jian and Wang, Jun},
doi = {10.1101/gr.097261.109},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2010 - De novo assembly of human genomes with massively parallel short read sequencing.pdf:pdf},
isbn = {8675525274247},
issn = {10889051},
journal = {Genome Research},
number = {2},
pages = {265--272},
pmid = {20019144},
title = {{De novo assembly of human genomes with massively parallel short read sequencing}},
volume = {20},
year = {2010}
}
@article{Koren2012a,
author = {Biotechnology, Nature and Mccombie, Richard and Spring, Cold},
doi = {10.1038/nbt.2280},
file = {:home/morispi1/Cours/PhD/Articles/Correction/PBcR.pdf:pdf},
number = {February 2017},
title = {{Hybrid error correction de novo assembly of single-molecule sequencing reads}},
year = {2012}
}
@article{Schulz2014,
abstract = {Motivation: Automatic error correction of high-throughput sequencing data can have a dramatic impact on the amount of usable base pairs and their quality. It has been shown that the performance of tasks such as de novo genome assembly and SNP calling can be dramatically improved after read error correction. While a large number of methods specialized for correcting substitution errors as found in Illumina data exist, few methods for the correction of indel errors, common to technologies like 454 or Ion Torrent, have been proposed. Results: We present Fiona, a new stand-alone read error-correction method. Fiona provides a new statistical approach for sequencing error detection and optimal error correction and estimates its parameters automatically. Fiona is able to correct substitution, insertion and deletion errors and can be applied to any sequencing technology. It uses an efficient implementation of the partial suffix array to detect read overlaps with different seed lengths in parallel. We tested Fiona on several real datasets from a variety of organisms with different read lengths and compared its performance with state-of-the-art methods. Fiona shows a constantly higher correction accuracy over a broad range of datasets from 454 and Ion Torrent sequencers, without compromise in speed. Conclusion: Fiona is an accurate parameter-free read error-correction method that can be run on inexpensive hardware and can make use of multicore parallelization whenever available. Fiona was implemented using the SeqAn library for sequence analysis and is publicly available for download at http://www.seqan.de/projects/fiona. Contact: mschulz@mmci.uni-saarland.de or hugues.richard@upmc.fr Supplementary information: Supplementary data are available at Bioinformatics online.},
author = {Schulz, Marcel H and Weese, David and Holtgrewe, Manuel and Dimitrova, Viktoria and Niu, Sijia and Reinert, Knut and Richard, Hugues},
doi = {10.1093/bioinformatics/btu440},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulz et al. - 2014 - Fiona A parallel and automatic strategy for read error correction(2).pdf:pdf},
isbn = {1367-4811 (Electronic){\$}\backslash{\$}r1367-4803 (Linking)},
issn = {14602059},
journal = {Bioinformatics},
number = {17},
pages = {356--363},
pmid = {25161220},
title = {{Fiona: A parallel and automatic strategy for read error correction}},
volume = {30},
year = {2014}
}
@article{Bloom1970,
abstract = {In this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash-coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency. The new methods are intended to reduce the amount of space required to contain the hash-coded information from that associated with conventional methods. The reduction in space is accomplished by exploiting the possibility that a small fraction of errors of commission may be tolerable in some applications, in particular, applications in which a large amount of data is involved and a core resident hash area is consequently not feasible using conventional methods. In such applications, it is envisaged that overall performance could be improved by using a smaller core resident hash area in conjunction with the new methods and, when necessary, by using some secondary and perhaps time-consuming test to “catch” the small fraction of errors associated with the new methods. An example is discussed which illustrates possible areas of application for the new methods. Analysis of the paradigm problem demonstrates that allowing a small number of test messages to be falsely identified as members of the given set will permit a much smaller hash area to be used without increasing reject time.},
author = {Bloom, Burton H.},
doi = {10.1145/362686.362692},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bloom - 1970 - Spacetime trade-offs in hash coding with allowable errors.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {and phrases,hash addressing,hash coding,retrieval efficiency,retrieval trade-offs,scatter storage,searching,storage,storage layout},
number = {7},
pages = {422--426},
title = {{Space/time trade-offs in hash coding with allowable errors}},
volume = {13},
year = {1970}
}
@article{Koren2016,
abstract = {Long-read single-molecule sequencing has revolutionized de novo genome assembly and enabled the automated reconstruction of reference-quality genomes. However, given the relatively high error rates of such technologies, efficient and accurate assembly of large repeats and closely related haplotypes remains challenging. We address these issues with Canu, a complete reworking of Celera Assembler that is specifically designed for noisy single-molecule sequences. Canu introduces support for nanopore sequencing, halves depth-of-coverage requirements, and improves assembly continuity while simultaneously reducing runtime by an order of magnitude on large genomes. These advances result from new overlapping and assembly algorithms, including an adaptive overlapping strategy based on tf-idf weighted MinHash and a sparse assembly graph construction that avoids collapsing diverged repeats and haplotypes. We demonstrate that Canu can reliably assemble complete microbial genomes and near-complete eukaryotic chromosomes using either PacBio or Oxford Nanopore technologies, and achieves a contig NG50 of greater than 21 Mbp on both human and Drosophila melanogaster PacBio datasets. For assembly structures that cannot be linearly represented, Canu provides graph-based assembly outputs for analysis or integration with complementary phasing and scaffolding techniques. Canu source code and pre-compiled binaries are freely available under a GPLv2 license from https://github.com/marbl/canu.},
archivePrefix = {arXiv},
arxivId = {071282},
author = {Koren, Sergey and Walenz, Brian P. and Berlin, Konstantin and Miller, Jason R. and Phillippy, Adam M.},
doi = {10.1101/071282},
eprint = {071282},
file = {:home/morispi1/Cours/PhD/Articles/Assemblage/Canu.pdf:pdf},
issn = {1088-9051},
journal = {bioRxiv},
keywords = {de novo assembly,nanopore sequencing,single-molecule sequencing},
pmid = {28298431},
title = {{Canu: scalable and accurate long-read assembly via adaptive k-mer weighting and repeat separation}},
year = {2016}
}
@article{Ilie2013,
abstract = {MOTIVATION: High throughput next generation sequencing technologies enable increasingly fast and affordable sequencing of genomes and transcriptomes, with a broad range of applications. The quality of the sequencing data is crucial for all applications. A significant portion of the data produced contains errors and ever more efficient error correction programs are needed.{\$}\backslash{\$}n{\$}\backslash{\$}nRESULTS: We propose RACER (Rapid and Accurate Correction of Errors in Reads), a new software program for correcting errors in sequencing data. RACER has better error correcting performance than existing programs, is faster, and requires less memory. To support our claims, we performed extensive comparison with the existing leading programs on a variety of real datasets.{\$}\backslash{\$}n{\$}\backslash{\$}nAVAILABILITY: RACER is freely available for non-commercial use at www.csd.uwo.ca/{\{}{\~{}}{\}}ilie/RACER/.{\$}\backslash{\$}n{\$}\backslash{\$}nCONTACT: ilie@csd.uwo.ca.},
author = {Ilie, Lucian and Molnar, Michael},
doi = {10.1093/bioinformatics/btt407},
file = {:home/morispi1/Cours/PhD/Articles/Correction/RACER (suppl).pdf:pdf},
isbn = {1367-4811 (Electronic){\$}\backslash{\$}r1367-4803 (Linking)},
issn = {14602059},
journal = {Bioinformatics},
number = {19},
pages = {2490--2493},
pmid = {23853064},
title = {{RACER: Rapid and accurate correction of errors in reads}},
volume = {29},
year = {2013}
}
@article{Kent2002,
abstract = {Analyzing vertebrate genomes requires rapid mRNA/DNA and cross-species protein alignments. A new tool, BLAT, is more accurate and 500 times faster than popular existing tools for mRNA/DNA alignments and 50 times faster for protein alignments at sensitivity settings typically used when comparing vertebrate sequences. BLAT's speed stems from an index of all nonoverlapping K-mers in the genome. This index fits inside the RAM of inexpensive computers, and need only be computed once for each genome assembly. BLAT has several major stages. It uses the index to find regions in the genome likely to be homologous to the query sequence. It performs an alignment between homologous regions. It stitches together these aligned regions (often exons) into larger alignments (typically genes). Finally, BLAT revisits small internal exons possibly missed at the first stage and adjusts large gap boundaries that have canonical splice sites where feasible. This paper describes how BLAT was optimized. Effects on speed and sensitivity are explored for various K-mer sizes, mismatch schemes, and number of required index matches. BLAT is compared with other alignment programs on various test sets and then used in several genome-wide applications. http://genome.ucsc.edu hosts a web-based BLAT server for the human genome. Some},
author = {Kent, W James},
doi = {10.1101/gr.229202.},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kent - 2002 - BLAT — The BLAST -Like Alignment Tool.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
pages = {656--664},
title = {{BLAT — The BLAST -Like Alignment Tool}},
volume = {12},
year = {2002}
}
@article{Delcher1999,
author = {Delcher, Arthur L and Kasif, Simon and Fleischmann, Robert D and Peterson, Jeremy and White, Owen and Salzberg, Steven L},
file = {:home/morispi1/Cours/PhD/Articles/Alignement/MUMmer.pdf:pdf},
number = {11},
pages = {2369--2376},
title = {{Alignment of whole genomes}},
volume = {27},
year = {1999}
}
@article{Li2015,
abstract = {Motivation: Single Molecule Real-Time (SMRT) sequencing technology and Oxford Nanopore technologies (ONT) produce reads over 10kbp in length, which have enabled high-quality genome assembly at an affordable cost. However, at present, long reads have an error rate as high as 10-15{\%}. Complex and computationally intensive pipelines are required to assemble such reads. Results: We present a new mapper, minimap, and a de novo assembler, miniasm, for efficiently mapping and assembling SMRT and ONT reads without an error correction stage. They can often assemble a sequencing run of bacterial data into a single contig in a few minutes, and assemble 45-fold C. elegans data in 9 minutes, orders of magnitude faster than the existing pipelines. We also introduce a pairwise read mapping format (PAF) and a graphical fragment assembly format (GFA), and demonstrate the interoperability between ours and current tools. Availability and implementation: https://github.com/lh3/minimap and https://github.com/lh3/miniasm Contact: hengli@broadinstitute.org},
archivePrefix = {arXiv},
arxivId = {1512.01801},
author = {Li, Heng},
doi = {10.1093/bioinformatics/btw152},
eprint = {1512.01801},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li - 2015 - Minimap and miniasm fast mapping and de novo assembly for noisy long sequences.pdf:pdf},
issn = {1367-4803},
journal = {arXiv},
pages = {1--7},
title = {{Minimap and miniasm: fast mapping and de novo assembly for noisy long sequences}},
url = {http://arxiv.org/abs/1512.01801},
year = {2015}
}
@article{Li2009,
abstract = {SUMMARY: SOAP2 is a significantly improved version of the short oligonucleotide alignment program that both reduces computer memory usage and increases alignment speed at an unprecedented rate. We used a Burrows Wheeler Transformation (BWT) compression index to substitute the seed strategy for indexing the reference sequence in the main memory. We tested it on the whole human genome and found that this new algorithm reduced memory usage from 14.7 to 5.4 GB and improved alignment speed by 20-30 times. SOAP2 is compatible with both single- and paired-end reads. Additionally, this tool now supports multiple text and compressed file formats. A consensus builder has also been developed for consensus assembly and SNP detection from alignment of short reads on a reference genome. AVAILABILITY: http://soap.genomics.org.cn.},
author = {Li, Ruiqiang and Yu, Chang and Li, Yingrui and Lam, Tak Wah and Yiu, Siu Ming and Kristiansen, Karsten and Wang, Jun},
doi = {10.1093/bioinformatics/btp336},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2009 - SOAP2 An improved ultrafast tool for short read alignment.pdf:pdf},
isbn = {1367-4811 (Electronic){\$}\backslash{\$}r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {15},
pages = {1966--1967},
pmid = {19497933},
title = {{SOAP2: An improved ultrafast tool for short read alignment}},
volume = {25},
year = {2009}
}
@article{Allam2015,
author = {Allam, Amin and Kalnis, Panos and Solovyev, Victor},
doi = {10.1093/bioinformatics/btv415},
file = {:home/morispi1/Cours/PhD/Articles/Correction/Karect.pdf:pdf},
journal = {Bioinformatics},
number = {21},
pages = {3421--3428},
title = {{Karect: accurate correction of substitution, insertion and deletion errors for next-generation sequencing data}},
volume = {31},
year = {2015}
}
@article{Weiner1973,
abstract = {In 1970, Knuth, Pratt, and Morris [1] showed how to do basic pattern matching in linear time. Related problems, such as those discussed in [4], have previously been solved by efficient but sub-optimal algorithms. In this paper, we introduce an interesting data structure called a bi-tree. A linear time algorithm for obtaining a compacted version of a bi-tree associated with a given string is presented. With this construction as the basic tool, we indicate how to solve several pattern matching problems, including some from [4] in linear time.},
author = {Weiner, Peter},
doi = {10.1109/SWAT.1973.13},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weiner - 1973 - Linear pattern matching algorithms.pdf:pdf},
issn = {0272-4847},
journal = {Switching and Automata Theory, 1973. SWAT '08. IEEE Conference Record of 14th Annual Symposium on},
pages = {1--11},
title = {{Linear pattern matching algorithms}},
year = {1973}
}
@article{Li2008a,
author = {Li, Heng and Ruan, Jue and Durbin, Richard and Li, Heng and Ruan, Jue and Durbin, Richard},
doi = {10.1101/gr.078212.108},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2008 - Mapping short DNA sequencing reads and calling variants using mapping quality scores Mapping short DNA sequencing rea.pdf:pdf},
pages = {1851--1858},
title = {{Mapping short DNA sequencing reads and calling variants using mapping quality scores Mapping short DNA sequencing reads and calling variants using mapping quality scores}},
year = {2008}
}
@article{B??inda2015,
abstract = {MOTIVATION: Metagenomics is a powerful approach to study genetic content of environmental samples, which has been strongly promoted by NGS technologies. To cope with massive data involved in modern metagenomic projects, recent tools (Ames et al., 2013; Wood and Salzberg, 2014) rely on the analysis of k-mers shared between the read to be classified and sampled reference genomes. RESULTS: Within this general framework, we show that spaced seeds provide a significant improvement of classification accuracy, as opposed to traditional contiguous k-mers. We support this thesis through a series a different computational experiments, including simulations of large-scale metagenomic projects.Availability and implementation, Supplementary information: Scripts and programs used in this study, as well as supplementary material, are available from http://github.com/gregorykucherov/spaced-seeds-for-metagenomics. CONTACT: gregory.kucherov@univ-mlv.fr.},
archivePrefix = {arXiv},
arxivId = {1502.06256},
author = {B??inda, Karel and Sykulski, MacIej and Kucherov, Gregory},
doi = {10.1093/bioinformatics/btv419},
eprint = {1502.06256},
file = {:home/morispi1/Cours/PhD/Articles/Divers/Spaced{\_}seeds{\_}improve{\_}kmersbased{\_}metagenomic{\_}classification.pdf:pdf},
issn = {14602059},
journal = {Bioinformatics},
number = {22},
pages = {3584--3592},
pmid = {26209798},
title = {{Spaced seeds improve k-mer-based metagenomic classification}},
volume = {31},
year = {2015}
}
@article{Au2012,
abstract = {The recent development of third generation sequencing (TGS) generates much longer reads than second generation sequencing (SGS) and thus provides a chance to solve problems that are difficult to study through SGS alone. However, higher raw read error rates are an intrinsic drawback in most TGS technologies. Here we present a computational method, LSC, to perform error correction of TGS long reads (LR) by SGS short reads (SR). Aiming to reduce the error rate in homopolymer runs in the main TGS platform, the PacBio{\textregistered} RS, LSC applies a homopolymer compression (HC) transformation strategy to increase the sensitivity of SR-LR alignment without scarifying alignment accuracy. We applied LSC to 100,000 PacBio long reads from human brain cerebellum RNA-seq data and 64 million single-end 75 bp reads from human brain RNA-seq data. The results show LSC can correct PacBio long reads to reduce the error rate by more than 3 folds. The improved accuracy greatly benefits many downstream analyses, such as directional gene isoform detection in RNA-seq study. Compared with another hybrid correction tool, LSC can achieve over double the sensitivity and similar specificity.},
author = {Au, Kin Fai and Underwood, Jason G. and Lee, Lawrence and Wong, Wing Hung},
doi = {10.1371/journal.pone.0046679},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Au et al. - 2012 - Improving PacBio Long Read Accuracy by Short Read Alignment.pdf:pdf},
isbn = {1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
number = {10},
pages = {1--8},
pmid = {23056399},
title = {{Improving PacBio Long Read Accuracy by Short Read Alignment}},
volume = {7},
year = {2012}
}
@article{Marcais2012,
abstract = {Jelly sh is a software to count k-mers in DNA sequences.},
author = {Marcais, G and Kingsford, C},
file = {:home/morispi1/Cours/PhD/Articles/kmers/jellyfish-manual-1.1.pdf:pdf},
number = {1},
pages = {1--8},
title = {{Jellyfish : A fast k-mer counter}},
year = {2012}
}
@article{Burrows1994,
abstract = {We describe a block-sorting, lossless data compression algorithm, and our imple- mentation of that algorithm. We compare the performance of our implementation with widely available data compressors running on the same hardware. The algorithmworks by applying a reversible transformation to a block of input text. The transformation does not itself compress the data, but reorders it to make it easy to compress with simple algorithms such as move-to-front coding. Ouralgorithm achieves speed comparable to algorithmsbased on the techniques of Lempel and Ziv, but obtains compression close to the best statisticalmodelling techniques. The size of the input block must be large (a few kilobytes) to achieve good compression.},
archivePrefix = {arXiv},
arxivId = {0908.0239},
author = {Burrows, M and Wheeler, Dj},
doi = {10.1.1.37.6774},
eprint = {0908.0239},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burrows, Wheeler - 1994 - A block-sorting lossless data compression algorithm.pdf:pdf},
isbn = {0769518966},
issn = {15708667},
journal = {Research report},
title = {{A block-sorting lossless data compression algorithm}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.141.5254},
year = {1994}
}
@article{Chin2013,
abstract = {We present a hierarchical genome-assembly process (HGAP) for high-quality de novo microbial genome assemblies using only a single, long-insert shotgun DNA library in conjunction with Single Molecule, Real-Time (SMRT) DNA sequencing. Our method uses the longest reads as seeds to recruit all other reads for construction of highly accurate preassembled reads through a directed acyclic graph–based consensus procedure, which we follow with assembly using off-the-shelf long-read assemblers. In contrast to hybrid approaches, HGAP does not require highly accurate raw reads for error correction. We demonstrate efficient genome assembly for several microorganisms using as few as three SMRT Cell zero- mode waveguide arrays of sequencing and for BACs using just one SMRT Cell. Long repeat regions can be successfully resolved with this workflow. We also describe a consensus algorithm that incorporates SMRT sequencing primary quality values to produce de novo genome sequence exceeding 99.999{\%} accuracy.},
author = {Chin, Chen-Shan and Alexander, David H and Marks, Patrick and Klammer, Aaron A and Drake, James and Heiner, Cheryl and Clum, Alicia and Copeland, Alex and Huddleston, John and Eichler, Evan E and Turner, Stephen W and Korlach, Jonas},
doi = {10.1038/nmeth.2474},
file = {:home/morispi1/Cours/PhD/Articles/Correction/HGAP.pdf:pdf},
isbn = {1548-7091},
issn = {1548-7091},
journal = {Nature Methods},
number = {6},
pages = {563--569},
pmid = {23644548},
title = {{Nonhybrid, finished microbial genome assemblies from long-read SMRT sequencing data}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.2474},
volume = {10},
year = {2013}
}
@article{Berlin2015,
abstract = {Long-read, single-molecule real-time (SMRT) sequencing is routinely used to finish microbial genomes, but available assembly methods have not scaled well to larger genomes. We introduce the MinHash Alignment Process (MHAP) for overlapping noisy, long reads using probabilistic, locality-sensitive hashing. Integrating MHAP with the Celera Assembler enabled reference-grade de novo assemblies of Saccharomyces cerevisiae, Arabidopsis thaliana, Drosophila melanogaster and a human hydatidiform mole cell line (CHM1) from SMRT sequencing. The resulting assemblies are highly continuous, include fully resolved chromosome arms and close persistent gaps in these reference genomes. Our assembly of D. melanogaster revealed previously unknown heterochromatic and telomeric transition sequences, and we assembled low-complexity sequences from CHM1 that fill gaps in the human GRCh38 reference. Using MHAP and the Celera Assembler, single-molecule sequencing can produce de novo near-complete eukaryotic assemblies that are 99.99{\%} accurate when compared with available reference genomes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1310.0883v1},
author = {Berlin, Konstantin and Koren, Sergey and Chin, Chen-Shan and Drake, James P and Landolin, Jane M and Phillippy, Adam M},
doi = {10.1038/nbt.3238},
eprint = {arXiv:1310.0883v1},
file = {:home/morispi1/Cours/PhD/Articles/Correction/MHAP.pdf:pdf},
isbn = {1087-0156},
issn = {1546-1696},
journal = {Nature biotechnology},
number = {6},
pages = {623--630},
pmid = {26006009},
title = {{Assembling large genomes with single-molecule sequencing and locality-sensitive hashing.}},
url = {http://dx.doi.org/10.1038/nbt.3238},
volume = {33},
year = {2015}
}
@article{Manber1991,
author = {Manber, Udi and Myers, Gene},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Manber, Myers - 1990 - Suffix Arrays A New Method for On-line String Searches.pdf:pdf},
journal = {Proceedings of the First Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {319--327},
title = {{Suffix Arrays: A New Method for On-line String Searches}},
volume = {SODA '90},
year = {1990}
}
@article{Li2010,
abstract = {Rapidly evolving sequencing technologies produce data on an unparalleled scale. A central challenge to the analysis of this data is sequence alignment, whereby sequence reads must be compared to a reference. A wide variety of alignment algorithms and software have been subsequently developed over the past two years. In this article, we will systematically review the current development of these algorithms and introduce their practical applications on different types of experimental data. We come to the conclusion that short-read alignment is no longer the bottleneck of data analyses. We also consider future development of alignment algorithms with respect to emerging long sequence reads and the prospect of cloud computing.},
author = {Li, Heng and Homer, Nils},
doi = {10.1093/bib/bbq015},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Homer - 2010 - A survey of sequence alignment algorithms for next-generation sequencing.pdf:pdf},
isbn = {1111111111},
issn = {14675463},
journal = {Briefings in Bioinformatics},
keywords = {Alignment algorithm,New sequencing technologies,Sequence analysis},
number = {5},
pages = {473--483},
pmid = {20460430},
title = {{A survey of sequence alignment algorithms for next-generation sequencing}},
volume = {11},
year = {2010}
}
@article{Danek2014,
abstract = {Motivation: The availability of thousands of invidual genomes of one species should boost rapid progress in personalized medicine or understanding of the interaction between genotype and phenotype, to name a few applications. A key operation useful in such analyses is aligning sequencing reads against a collection of genomes, which is costly with the use of existing algorithms due to their large memory requirements. Results: We present MuGI, Multiple Genome Index, which reports all occurrences of a given pattern, in exact and approximate matching model, against a collection of thousand(s) genomes. Its unique feature is the small index size fitting in a standard computer with 16--32$\backslash$,GB, or even 8$\backslash$,GB, of RAM, for the 1000GP collection of 1092 diploid human genomes. The solution is also fast. For example, the exact matching queries are handled in average time of 39$\backslash$,{\$}\backslashmu{\$}s and with up to 3 mismatches in 373$\backslash$,{\$}\backslashmu{\$}s on the test PC with the index size of 13.4$\backslash$,GB. For a smaller index, occupying 7.4$\backslash$,GB in memory, the respective times grow to 76$\backslash$,{\$}\backslashmu{\$}s and 917$\backslash$,{\$}\backslashmu{\$}s. Availability: Software and Suuplementary material: $\backslash$url{\{}http://sun.aei.polsl.pl/mugi{\}}.},
archivePrefix = {arXiv},
arxivId = {1403.7481},
author = {Danek, Agnieszka and Deorowicz, Sebastian and Grabowski, Szymon},
doi = {10.1371/journal.pone.0109384},
eprint = {1403.7481},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Danek, Deorowicz, Grabowski - 2014 - Indexes of large genome collections on a PC.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {10},
pages = {1--7},
pmid = {25289699},
title = {{Indexes of large genome collections on a PC}},
volume = {9},
year = {2014}
}
@article{Schroder2009,
abstract = {MOTIVATION: Second-generation sequencing technologies produce a massive amount of short reads in a single experiment. However, sequencing errors can cause major problems when using this approach for de novo sequencing applications. Moreover, existing error correction methods have been designed and optimized for shotgun sequencing. Therefore, there is an urgent need for the design of fast and accurate computational methods and tools for error correction of large amounts of short read data. RESULTS: We present SHREC, a new algorithm for correcting errors in short-read data that uses a generalized suffix trie on the read data as the underlying data structure. Our results show that the method can identify erroneous reads with sensitivity and specificity of over 99{\{}{\{}{\}}{\{}{\%}{\}}{\{}{\}}{\}} and 96{\{}{\{}{\}}{\{}{\%}{\}}{\{}{\}}{\}} for simulated data with error rates of up to 3{\{}{\{}{\}}{\{}{\%}{\}}{\{}{\}}{\}} as well as for real data. Furthermore, it achieves an error correction accuracy of over 80{\{}{\{}{\}}{\{}{\%}{\}}{\{}{\}}{\}} for simulated data and over 88{\{}{\{}{\}}{\{}{\%}{\}}{\{}{\}}{\}} for real data. These results are clearly superior to previously published approaches. SHREC is available as an efficient open-source Java implementation that allows processing of 10 million of short reads on a standard workstation.},
author = {Schr{\"{o}}der, Jan and Schr{\"{o}}der, Heiko and Puglisi, Simon J and Sinha, Ranjan and Schmidt, Bertil},
doi = {10.1093/bioinformatics/btp379},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schr{\"{o}}der et al. - 2009 - SHREC A short-read error correction method.pdf:pdf},
isbn = {1367-4811 (Electronic){\{}{\$}{\}}\backslashbackslash{\{}\backslash{\$}{\}}r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {17},
pages = {2157--2163},
pmid = {19542152},
title = {{SHREC: A short-read error correction method}},
volume = {25},
year = {2009}
}
@article{Salmela2011,
abstract = {Current sequencing technologies produce a large number of erroneous reads. The sequencing errors present a major challenge in utilizing the data in de novo sequencing projects as assemblers have difficulties in dealing with errors.},
author = {Salmela, Leena and Schr{\"{o}}der, Jan},
doi = {10.1093/bioinformatics/btr170},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salmela, Schr{\"{o}}der - 2011 - Correcting errors in short reads by multiple alignments.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {11},
pages = {1455--1461},
pmid = {21471014},
title = {{Correcting errors in short reads by multiple alignments}},
volume = {27},
year = {2011}
}
@article{Ferragina2000a,
abstract = {We address the issue of compressing and indexing data. We devise a data structure whose space occupancy is a function of the entropy of the underlying data set. We call the data structure opportunistic since its space occupancy is decreased when the input is compressible and this space reduction is achieved at no significant slowdown in the query performance. More precisely, its space occupancy is optimal in an information-content sense because text T1,u is stored using O(Hk (T))+o(1) bits per input symbol in the worst case, where Hk (T) is the kth order empirical entropy of T (the bound holds for any fixed k). Given an arbitrary string P1,p, the opportunistic data structure allows to search for the occurrences of P in T in time (for any fixed {\&}949;{\&}gt;0). If data are uncompressible we achieve the best space bound currently known (Grossi and Vitter, 2000); on compressible data our solution improves the succinct suffix array of (Grossi and Vitter, 2000) and the classical suffix tree and suffix array data structures either in space or in query time or both. We also study our opportunistic data structure in a dynamic setting and devise a variant achieving effective search and update time bounds. Finally, we show how to plug our opportunistic data structure into the Glimpse tool (Manber and Wu, 1994). The result is an indexing tool which achieves sublinear space and sublinear query time complexity},
author = {Ferragina, P and Manzini, G},
doi = {10.1109/SFCS.2000.892127},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ferragina, Manzini - 2000 - Opportunistic data structures with applications.pdf:pdf},
isbn = {0-7695-0850-2},
issn = {0272-5428},
journal = {Proceedings of the 41st Annual Symposium on Foundations of Computer Science},
keywords = {Computer science,Costs,Data engineering,Fault tolerance,Glimpse tool,Indexing,Plugs,Postal services,Tree data structures,computational complexity,data compression,data indexing,data set,data structures,database indexing,database theory,entropy,opportunistic data structures,query performance,search,sublinear query time complexity,sublinear space complexity,succinct suffix array,suffix array data structures,suffix tree data structures},
pages = {390--398},
title = {{Opportunistic data structures with applications}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=892127{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=892127{\%}5Cnhttp://dl.acm.org/citation.cfm?id=796543},
volume = {FOCS '00},
year = {2000}
}
@article{Laehnemann2016,
abstract = {Characterizing the errors generated by common high-throughput sequencing platforms and telling true genetic variation from technical artefacts are two interdependent steps, essential to many analyses such as single nucleotide variant calling, haplotype inference, sequence assembly and evolutionary studies. Both random and systematic errors can show a specific occurrence profile for each of the six prominent sequencing platforms surveyed here: 454 pyrosequencing, Complete Genomics DNA nanoball sequencing, Illumina sequencing by synthesis, Ion Torrent semiconductor sequencing, Pacific Biosciences single-molecule real-time sequencing and Oxford Nanopore sequencing. There is a large variety of programs available for error removal in sequencing read data, which differ in the error models and statistical techniques they use, the features of the data they analyse, the parameters they determine from them and the data structures and algorithms they use. We highlight the assumptions they make and for which data types these hold, providing guidance which tools to consider for benchmarking with regard to the data properties. While no benchmarking results are included here, such specific benchmarks would greatly inform tool choices and future software development. The development of stand-alone error correctors, as well as single nucleotide variant and haplotype callers, could also benefit from using more of the knowledge about error profiles and from (re)combining ideas from the existing approaches presented here.},
author = {Laehnemann, David and Borkhardt, Arndt and McHardy, Alice Carolyn},
doi = {10.1093/bib/bbv029},
file = {:home/morispi1/Cours/PhD/Articles/Correction/Survey (2016).pdf:pdf},
isbn = {1477-4054 (Electronic)$\backslash$r1467-5463 (Linking)},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Bias,Error correction,Error model,Error profile,High-throughput sequencing,Next-generation sequencing},
number = {1},
pages = {154--179},
pmid = {26026159},
title = {{Denoising DNA deep sequencing data-high-throughput sequencing errors and their correction}},
volume = {17},
year = {2016}
}
@article{Hach2010,
author = {Hach, Faraz and Hormozdiari, Fereydoun and Alkan, Can and Hormozdiari, Farhad and Birol, Inanc and Eichler, Evan E and Sahinalp, S Cenk},
doi = {10.1038/nmeth0810-576},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hach et al. - 2010 - mrsFAST a cache-oblivious algorithm for short-read mapping.pdf:pdf},
issn = {1548-7091},
journal = {Nature Methods},
number = {8},
pages = {576--577},
pmid = {20676076},
title = {{mrsFAST: a cache-oblivious algorithm for short-read mapping}},
url = {http://www.nature.com/doifinder/10.1038/nmeth0810-576},
volume = {7},
year = {2010}
}
@article{Salmela2016,
abstract = {Motivation: New long read sequencing technologies, like PacBio SMRT and Oxford NanoPore, can produce sequencing reads up to 50,000 bp long but with an error rate of at least 15{\%}. Reducing the error rate is necessary for subsequent utilisation of the reads in, e.g., de novo genome assembly. The error correction problem has been tackled either by aligning the long reads against each other or by a hybrid approach that uses the more accurate short reads produced by second generation sequencing technologies to correct the long reads. Results: We present an error correction method that uses long reads only. The method consists of two phases: first we use an iterative alignment-free correction method based on de Bruijn graphs with increasing length of k-mers, and second, the corrected reads are further polished using long-distance dependencies that are found using multiple alignments. According to our experiments the proposed method is the most accurate one relying on long reads only for read sets with high coverage. Furthermore, when the coverage of the read set is at least 75x, the throughput of the new method is at least 20{\%} higher. Availability: LoRMA is freely available at http://www.cs.helsinki.fi/u/lmsalmel/LoRMA/. Contact: leena.salmela@cs.helsinki.fi},
archivePrefix = {arXiv},
arxivId = {1604.02233},
author = {Salmela, Leena and Walve, Riku and Rivals, Eric and Ukkonen, Esko},
doi = {10.1093/bioinformatics/btw321},
eprint = {1604.02233},
file = {:home/morispi1/Cours/PhD/Articles/Correction/LoRMA.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {1367-4803},
journal = {Bioinformatics},
number = {6},
pages = {799--806},
pmid = {27273673},
title = {{Accurate selfcorrection of errors in long reads using de Bruijn graphs}},
url = {http://bioinformatics.oxfordjournals.org/content/early/2016/06/03/bioinformatics.btw321{\%}5Cnhttp://bioinformatics.oxfordjournals.org/content/early/2016/06/03/bioinformatics.btw321.full.pdf{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/27273673{\%}5Cnhttps://academic.},
volume = {33},
year = {2017}
}
@article{Langmead2009,
abstract = {Bowtie is an ultrafast, memory-efficient alignment program for aligning short DNA sequence reads to large genomes. For the human genome, Burrows-Wheeler indexing allows Bowtie to align more than 25 million reads per CPU hour with a memory footprint of approximately 1.3 gigabytes. Bowtie extends previous Burrows-Wheeler techniques with a novel quality-aware backtracking algorithm that permits mismatches. Multiple processor cores can be used simultaneously to achieve even greater alignment speeds. Bowtie is open source (http://bowtie.cbcb.umd.edu).},
author = {Langmead, B and Trapnell, C and Pop, M and Salzberg, S L},
doi = {gb-2009-10-3-r25 [pii]\n10.1186/gb-2009-10-3-r25},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Langmead et al. - 2009 - Ultrafast and memory-efficient alignment of short DNA sequences to the human genome.pdf:pdf},
isbn = {1465-6914 (Electronic){\$}\backslash{\$}n1465-6906 (Linking)},
issn = {1465-6914; 1465-6906},
journal = {Genome biology},
keywords = {*Base Sequence,Algorithms,Genome,Human/*genetics,Humans,Sequence Alignment/*methods},
number = {3},
pages = {R25},
pmid = {19261174},
title = {{Ultrafast and memory-efficient alignment of short DNA sequences to the human genome}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve{\%}7B{\&}{\%}7Ddb=PubMed{\%}7B{\&}{\%}7Ddopt=Citation{\%}7B{\&}{\%}7Dlist{\%}7B{\_}{\%}7Duids=19261174},
volume = {10},
year = {2009}
}
@article{Deorowicz2014,
abstract = {Motivation: Building the histogram of occurrences of every {\$}k{\$}-symbol long substring of nucleotide data is a standard step in many bioinformatics applications, known under the name of {\$}k{\$}-mer counting. Its applications include developing de Bruijn graph genome assemblers, fast multiple sequence alignment and repeat detection. The tremendous amounts of NGS data require fast algorithms for {\$}k{\$}-mer counting, preferably using moderate amounts of memory. Results: We present a novel method for {\$}k{\$}-mer counting, on large datasets at least twice faster than the strongest competitors (Jellyfish{\~{}}2, KMC{\~{}}1), using about 12$\backslash$,GB (or less) of RAM memory. Our disk-based method bears some resemblance to MSPKmerCounter, yet replacing the original minimizers with signatures (a carefully selected subset of all minimizers) and using {\$}(k, x){\$}-mers allows to significantly reduce the I/O, and a highly parallel overall architecture allows to achieve unprecedented processing speeds. For example, KMC{\~{}}2 allows to count the 28-mers of a human reads collection with 44-fold coverage (106$\backslash$,GB of compressed size) in about 20 minutes, on a 6-core Intel i7 PC with an SSD. Availability: KMC{\~{}}2 is freely available at http://sun.aei.polsl.pl/kmc. Contact: sebastian.deorowicz@polsl.pl},
archivePrefix = {arXiv},
arxivId = {arXiv:1407.1507v1},
author = {Deorowicz, Sebastian and Kokot, Marek and Grabowski, Szymon and Debudaj-Grabysz, Agnieszka},
doi = {10.1093/bioinformatics/btv022},
eprint = {arXiv:1407.1507v1},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deorowicz et al. - 2014 - KMC 2 Fast and resource-frugal k-mer counting.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {14602059},
journal = {Bioinformatics},
number = {10},
pages = {1569--1576},
pmid = {25609798},
title = {{KMC 2: Fast and resource-frugal k-mer counting}},
volume = {31},
year = {2014}
}
@article{Sam2015,
author = {Sam, The and Format, B A M and Working, Specification},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sam, Format, Working - 2015 - SAMv1.pdf:pdf},
isbn = {8901234567890},
pages = {1--16},
title = {{SAMv1}},
year = {2015}
}
@article{Kurtz2004,
author = {Access, Open},
file = {:home/morispi1/Cours/PhD/Articles/Alignement/dnadiff.pdf:pdf},
number = {2},
title = {{Versatile and open software for comparing large genomes}},
volume = {5},
year = {2004}
}
@article{Koren2012,
abstract = {Single-molecule sequencing instruments can generate multikilobase sequences with the potential to greatly improve genome and transcriptome assembly. However, the error rates of single-molecule reads are high, which has limited their use thus far to resequencing bacteria. To address this limitation, we introduce a correction algorithm and assembly strategy that uses short, high-fidelity sequences to correct the error in single-molecule sequences. We demonstrate the utility of this approach on reads generated by a PacBio RS instrument from phage, prokaryotic and eukaryotic whole genomes, including the previously unsequenced genome of the parrot Melopsittacus undulatus, as well as for RNA-Seq reads of the corn (Zea mays) transcriptome. Our long-read correction achieves {\textgreater}99.9{\%} base-call accuracy, leading to substantially better assemblies than current sequencing strategies: in the best example, the median contig size was quintupled relative to high-coverage, second-generation assemblies. Greater gains are predicted if read lengths continue to increase, including the prospect of single-contig bacterial chromosome assembly.},
author = {Koren, Sergey and Schatz, Michael C and Walenz, Brian P and Martin, Jeffrey and Howard, Jason T and Ganapathy, Ganeshkumar and Wang, Zhong and Rasko, David A and McCombie, W Richard and Jarvis, Erich D and {Adam M Phillippy}},
doi = {10.1038/nbt.2280},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koren et al. - 2012 - Hybrid error correction and de novo assembly of single-molecule sequencing reads.pdf:pdf},
isbn = {1546-1696 (Electronic)$\backslash$n1087-0156 (Linking)},
issn = {1546-1696},
journal = {Nature biotechnology},
keywords = {Algorithms,Bacteria,Bacteria: genetics,Bacteriophages,Bacteriophages: genetics,Computational Biology,Computational Biology: methods,RNA,RNA: genetics,Sequence Analysis, RNA,Sequence Analysis, RNA: methods,Transcriptome,Transcriptome: genetics,Zea mays,Zea mays: genetics},
number = {7},
pages = {693--700},
pmid = {22750884},
title = {{Hybrid error correction and de novo assembly of single-molecule sequencing reads.}},
url = {http://www.nature.com/nbt/journal/v30/n7/full/nbt.2280.html{\#}methods},
volume = {30},
year = {2012}
}
@article{Langmead2012,
abstract = {As the rate of sequencing increases, greater throughput is demanded from read aligners. The full-text minute index is often used to make alignment very fast and memory-efficient, but the approach is ill-suited to finding longer, gapped alignments. Bowtie 2 combines the strengths of the full-text minute index with the flexibility and speed of hardware-accelerated dynamic programming algorithms to achieve a combination of high speed, sensitivity and accuracy.},
archivePrefix = {arXiv},
arxivId = {{\{}{\#}{\}}14603},
author = {Langmead, Ben and Salzberg, Steven L},
doi = {10.1038/nmeth.1923},
eprint = {{\{}{\#}{\}}14603},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Langmead, Salzberg - 2012 - Fast gapped-read alignment with Bowtie 2.pdf:pdf},
isbn = {1548-7105 (Electronic) 1548-7091 (Linking)},
issn = {1548-7105},
journal = {Nat Methods},
keywords = {*Algorithms,Algorithms,Computational Biology,Computational Biology/*methods,Computational Biology: methods,DNA,DNA/methods,DNA: methods,Databases,Genetic,Genome,Human,Human/genetics,Human: genetics,Humans,Sequence Alignment,Sequence Alignment/*methods,Sequence Alignment: methods,Sequence Analysis},
number = {4},
pages = {357--359},
pmid = {22388286},
title = {{Fast gapped-read alignment with Bowtie 2}},
volume = {9},
year = {2012}
}
@article{Niko2013,
author = {V{\"{a}}lim{\"{a}}ki, Niko and Rivals, Eric},
journal = {Proceedings of the 9th International Symposium on Bioinformatics Research and Applications},
pages = {237--248},
title = {{Scalable and Versatile k -mer Indexing for High-Throughput Sequencing Data}},
year = {2013}
}
@article{Altschul1990,
abstract = {A new approach to rapid sequence comparison, basic local alignment search tool (BLAST), directly approximates alignments that optimize a measure of local similarity, the maximal segment pair (MSP) score. Recent mathematical results on the stochastic properties of MSP scores allow an analysis of the performance of this method as well as the statistical significance of alignments it generates. The basic algorithm is simple and robust; it can be implemented in a number of ways and applied in a variety of contexts including straightforward DNA and protein sequence database searches, motif searches, gene identification searches, and in the analysis of multiple regions of similarity in long DNA sequences. In addition to its flexibility and tractability to mathematical analysis, BLAST is an order of magnitude faster than existing sequence comparison tools of comparable sensitivity.},
author = {Altschul, S F and Gish, W and Miller, W and Myers, E W and Lipman, D J},
doi = {10.1016/S0022-2836(05)80360-2},
file = {:home/morispi1/Cours/PhD/Articles/Alignement/BLAST.pdf:pdf},
isbn = {0022-2836 (Print)},
issn = {0022-2836},
journal = {Journal of molecular biology},
keywords = {Algorithms,Amino Acid Sequence,Base Sequence,Databases, Factual,Mutation,Sensitivity and Specificity,Sequence Homology, Nucleic Acid,Software},
number = {3},
pages = {403--10},
pmid = {2231712},
title = {{Basic local alignment search tool.}},
url = {http://www.sciencedirect.com/science/article/pii/S0022283605803602},
volume = {215},
year = {1990}
}
@article{Salmela2010,
abstract = {MOTIVATION: High-throughput sequencing technologies produce large sets of short reads that may contain errors. These sequencing errors make de novo assembly challenging. Error correction aims to reduce the error rate prior assembly. Many de novo sequencing projects use reads from several sequencing technologies to get the benefits of all used technologies and to alleviate their shortcomings. However, combining such a mixed set of reads is problematic as many tools are specific to one sequencing platform. The SOLiD sequencing platform is especially problematic in this regard because of the two base color coding of the reads. Therefore, new tools for working with mixed read sets are needed. RESULTS: We present an error correction tool for correcting substitutions, insertions and deletions in a mixed set of reads produced by various sequencing platforms. We first develop a method for correcting reads from any sequencing technology producing base space reads such as the SOLEXA/Illumina and Roche/454 Life Sciences sequencing platforms. We then further refine the algorithm to correct the color space reads from the Applied Biosystems SOLiD sequencing platform together with normal base space reads. Our new tool is based on the SHREC program that is aimed at correcting SOLEXA/Illumina reads. Our experiments show that we can detect errors with 99{\{}{\%}{\}} sensitivity and {\{}{\textgreater}{\}}98{\{}{\%}{\}} specificity if the combined sequencing coverage of the sets is at least 12. We also show that the error rate of the reads is greatly reduced. AVAILABILITY: The JAVA source code is freely available at http://www.cs.helsinki.fi/u/lmsalmel/hybrid-shrec/ CONTACT: leena.salmela@cs.helsinki.fi},
author = {Salmela, Leena},
doi = {10.1093/bioinformatics/btq151},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salmela - 2010 - Correction of sequencing errors in a mixed set of reads.pdf:pdf},
isbn = {1367-4803},
issn = {13674803},
journal = {Bioinformatics},
number = {10},
pages = {1284--1290},
pmid = {20378555},
title = {{Correction of sequencing errors in a mixed set of reads}},
volume = {26},
year = {2010}
}
@article{Goodwin2015,
abstract = {Monitoring the progress of DNA through a pore has been postulated as a method for sequencing DNA for several decades1,2. Recently, a nanopore instrument, the Oxford Nanopore MinION, has become available3. Here we describe our sequencing of the S. cerevisiae genome. We describe software developed to make use of these data as existing packages were incapable of assembling long reads at such high error rate ({\{}{\~{}}{\}}35{\{}{\%}{\}} error). With these methods we were able to error correct and assemble the nanopore reads de novo, producing an assembly that is contiguous and accurate: with a contig N50 length of 479kb, and has greater than 99{\{}{\%}{\}} consensus identity when compared to the reference. The assembly with the long nanopore reads was able to correctly assemble gene cassettes, rRNAs, transposable elements, and other genomic features that were almost entirely absent in an assembly using Illumina sequencing alone (with a contig N50 of only 59,927bp).},
author = {Goodwin, Sara and Gurtowski, James and Ethe-Sayers, Scott and Deshpande, Panchajanya and Schatz, Michael and McCombie, W Richard},
doi = {10.1101/013490},
file = {:home/morispi1/Cours/PhD/Articles/Correction/Nanocorr.pdf:pdf},
isbn = {1088-9051},
issn = {1088-9051},
journal = {bioRxiv},
keywords = {Nanopore,de novo,genome assembly,oxford nanopore,single molecule sequencing,third generation sequencing,yeast},
pages = {13490},
pmid = {26447147},
title = {{Oxford Nanopore Sequencing and de novo Assembly of a Eukaryotic Genome}},
url = {http://biorxiv.org/content/early/2015/01/06/013490.abstract},
year = {2015}
}
@article{Philippe2011,
abstract = {BACKGROUND: High Throughput Sequencing (HTS) is now heavily exploited for genome (re-) sequencing, metagenomics, epigenomics, and transcriptomics and requires different, but computer intensive bioinformatic analyses. When a reference genome is available, mapping reads on it is the first step of this analysis. Read mapping programs owe their efficiency to the use of involved genome indexing data structures, like the Burrows-Wheeler transform. Recent solutions index both the genome, and the k-mers of the reads using hash-tables to further increase efficiency and accuracy. In various contexts (e.g. assembly or transcriptome analysis), read processing requires to determine the sub-collection of reads that are related to a given sequence, which is done by searching for some k-mers in the reads. Currently, many developments have focused on genome indexing structures for read mapping, but the question of read indexing remains broadly unexplored. However, the increase in sequence throughput urges for new algorithmic solutions to query large read collections efficiently. RESULTS: Here, we present a solution, named Gk arrays, to index large collections of reads, an algorithm to build the structure, and procedures to query it. Once constructed, the index structure is kept in main memory and is repeatedly accessed to answer queries like "given a k-mer, get the reads containing this k-mer (once/at least once)". We compared our structure to other solutions that adapt uncompressed indexing structures designed for long texts and show that it processes queries fast, while requiring much less memory. Our structure can thus handle larger read collections. We provide examples where such queries are adapted to different types of read analysis (SNP detection, assembly, RNA-Seq). CONCLUSIONS: Gk arrays constitute a versatile data structure that enables fast and more accurate read analysis in various contexts. The Gk arrays provide a flexible brick to design innovative programs that mine efficiently genomics, epigenomics, metagenomics, or transcriptomics reads. The Gk arrays library is available under Cecill (GPL compliant) license from http://www.atgc-montpellier.fr/ngs/.},
author = {Philippe, Nicolas and Salson, Mika{\"{e}}l and Lecroq, Thierry and L{\'{e}}onard, Martine and Commes, Th{\'{e}}r{\`{e}}se and Rivals, Eric},
doi = {10.1186/1471-2105-12-242},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Philippe et al. - 2011 - Querying large read collections in main memory a versatile data structure.pdf:pdf},
isbn = {1471-2105 (Electronic) 1471-2105 (Linking)},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {*Algorithms,*Computers,*High-Throughput Nucleotide Sequencing,Computational Biology/*methods,Humans,Software},
number = {1},
pages = {242},
pmid = {21682852},
title = {{Querying large read collections in main memory: a versatile data structure}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21682852},
volume = {12},
year = {2011}
}
@article{Kowalski2015,
abstract = {We propose a lightweight data structure for indexing and querying collections of NGS reads data in main memory. The data structure supports the interface proposed in the pioneering work by Philippe et al. for counting and locating k-mers in sequencing reads. Our solution, PgSA (pseudogenome suffix array), based on finding overlapping reads, is competitive to the existing algorithms in the space use, query times, or both. The main applications of our index include variant calling, error correction and analysis of reads from RNA-seq experiments.},
archivePrefix = {arXiv},
arxivId = {1502.01861},
author = {Kowalski, Tomasz and Grabowski, Szymon and Deorowicz, Sebastian},
doi = {10.1371/journal.pone.0133198},
eprint = {1502.01861},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kowalski, Grabowski, Deorowicz - 2015 - Indexing arbitrary-length k-mers in sequencing reads.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {7},
pages = {1--14},
pmid = {26182400},
title = {{Indexing arbitrary-length k-mers in sequencing reads}},
volume = {10},
year = {2015}
}
@article{Yang2013,
abstract = {Error Correction is important formost next-generation sequencing applications because highly accurate sequenced readswill likely lead to higher quality results.Many techniques for error correction of sequencing data fromnext-gen platforms have been developed in the recent years. However, compared with the fast development of sequencing technologies, there is a lack of standardized evaluation procedure for different error-correction methods, making it difficult to assess their relative merits and demerits. In this article, we provide a comprehensive review of many error-correctionmethods, and establish a common set of benchmark data and evaluation criteria to provide a com- parative assessment.We present experimental results on quality, run-time, memory usage and scalability of several error-correction methods. Apart from providing explicit recommendations useful to practitioners, the review serves to identify the current state of the art and promising directions for future research. Availability: All error-correction programs used in this article are downloaded fromhosting websites.The evaluation tool kit is pub- licly available at: http://aluru-sun.ece.iastate.edu/doku.php?id¼ecr},
author = {Yang, Xiao and Chockalingam, Sriram P. and Aluru, Srinivas},
doi = {10.1093/bib/bbs015},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Chockalingam, Aluru - 2013 - A survey of error-correction methods for next-generation sequencing.pdf:pdf},
isbn = {1467546314774},
issn = {14675463},
journal = {Briefings in Bioinformatics},
keywords = {Error correction,Next-generation sequencing,Sequence analysis},
number = {1},
pages = {56--66},
pmid = {22492192},
title = {{A survey of error-correction methods for next-generation sequencing}},
volume = {14},
year = {2013}
}
@article{Mottillo2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1401.4290v2},
author = {Mottillo, Cristina and Lu, Yuneng and Phan, Hao-min and Cliffe, Matthew J and Do, Trong-on},
doi = {10.9774/GLEAF.978-1-909493-77-3_30},
eprint = {arXiv:1401.4290v2},
file = {:home/morispi1/Cours/PhD/Articles/Correction/Colormap Supp.pdf:pdf},
isbn = {9781909493773},
issn = {01616412},
pmid = {17636916},
title = {{Supplementary Material}},
year = {2013}
}
@article{Hu2016,
author = {Hu, Ruifeng and Sun, Guibo and Sun, Xiaobo},
doi = {10.1186/s12859-016-1316-y},
file = {:home/morispi1/Cours/PhD/Articles/Correction/LSCPlus.PDF:PDF},
issn = {1471-2105},
journal = {BMC Bioinformatics},
keywords = {SMRT sequencing,RNA-seq,Error correction,Time-cons,error correction,rna-seq,smrt sequencing,time-consumption},
number = {1},
pages = {451},
pmid = {27829364},
publisher = {BMC Bioinformatics},
title = {{LSCplus: a fast solution for improving long read accuracy by short read alignment}},
url = {http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1316-y},
volume = {17},
year = {2016}
}
@article{Heo2014,
abstract = {MOTIVATION: Rapid advances in next-generation sequencing (NGS) technology have led to exponential increase in the amount of genomic information. However, NGS reads contain far more errors than data from traditional sequencing methods, and downstream genomic analysis results can be improved by correcting the errors. Unfortunately, all the previous error correction methods required a large amount of memory, making it unsuitable to process reads from large genomes with commodity computers.{\$}\backslash{\$}n{\$}\backslash{\$}nRESULTS: We present a novel algorithm that produces accurate correction results with much less memory compared with previous solutions. The algorithm, named BLoom-filter-based Error correction Solution for high-throughput Sequencing reads (BLESS), uses a single minimum-sized Bloom filter, and is also able to tolerate a higher false-positive rate, thus allowing us to correct errors with a 40× memory usage reduction on average compared with previous methods. Meanwhile, BLESS can extend reads like DNA assemblers to correct errors at the end of reads. Evaluations using real and simulated reads showed that BLESS could generate more accurate results than existing solutions. After errors were corrected using BLESS, 69{\{}{\%}{\}} of initially unaligned reads could be aligned correctly. Additionally, de novo assembly results became 50{\{}{\%}{\}} longer with 66{\{}{\%}{\}} fewer assembly errors.{\$}\backslash{\$}n{\$}\backslash{\$}nAVAILABILITY AND IMPLEMENTATION: Freely available at http://sourceforge.net/p/bless-ec CONTACT: dchen@illinois.edu SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Heo, Yun and Wu, Xiao Long and Chen, Deming and Ma, Jian and Hwu, Wen Mei},
doi = {10.1093/bioinformatics/btu030},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heo et al. - 2014 - BLESS Bloom filter-based error correction solution for high-throughput sequencing reads.pdf:pdf},
isbn = {1367-4811 (Electronic){\$}\backslash{\$}r1367-4803 (Linking)},
issn = {14602059},
journal = {Bioinformatics},
number = {10},
pages = {1354--1362},
pmid = {24451628},
title = {{BLESS: Bloom filter-based error correction solution for high-throughput sequencing reads}},
volume = {30},
year = {2014}
}
@article{Boden2013,
author = {Boden, Marcus and Sch{\"{o}}neich, Martin and Horwege, Sebastian},
doi = {10.4230/OASIcs.GCB.2013.24},
file = {:home/morispi1/Cours/PhD/Articles/kmers/boden-gcb.pdf:pdf},
isbn = {3939897590},
issn = {2190-6807},
journal = {German Conference on Bioinformatics 2013},
keywords = {2013,24,4230,and phrases alignment-free sequence,comparison,digital object identifier 10,gcb,oasics,phylogeny reconstruction},
pages = {24--34},
title = {{Alignment-free sequence comparison with spaced k-mers}},
volume = {34},
year = {2013}
}
@article{Correspondinga,
author = {Corresponding, Country},
file = {:home/morispi1/Cours/PhD/Articles/Moi/template{\_}jobim2017/Proceedings/LaTeX/HG-CoLoR.pdf:pdf},
keywords = {assembly,correction,long reads,ngs},
pages = {1--8},
title = {{HG-CoLoR: A new method for the production of synthetic long reads}}
}
@article{Philippe2010,
author = {Philippe, Nicolas and Lecroq, Thierry and Martine, L and Rivals, Eric},
file = {:home/morispi1/Cours/PhD/Articles/Requetes/Gk{\_}Arrays{\_}Suppl.pdf:pdf},
number = {line 10},
pages = {1--5},
title = {{Additional File 1 : Proof and queries algorithms to manuscript ” Querying huge read sets in main memory : a versatile data structure ” Additional table : elements of GkSA and their}},
year = {2010}
}
@article{Yang2010,
abstract = {Error correction is critical to the success of next-generation sequencing applications, such as resequencing and de novo genome sequencing. It is especially important for high-throughput short-read sequencing, where reads are much shorter and more abundant, and errors more frequent than in traditional Sanger sequencing. Processing massive numbers of short reads with existing error correction methods is both compute and memory intensive, yet the results are far from satisfactory when applied to real datasets.},
author = {Yang, Xiao and Dorman, Karin S and Aluru, Srinivas},
doi = {10.1093/bioinformatics/btq468},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Dorman, Aluru - 2010 - Reptile Representative tiling for short read error correction.pdf:pdf},
isbn = {1367-4811 (Electronic){\$}\backslash{\$}r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {20},
pages = {2526--2533},
pmid = {20834037},
title = {{Reptile: Representative tiling for short read error correction}},
volume = {26},
year = {2010}
}
@article{Chikhi2013,
author = {Chikhi, Rayan and Rizk, Guillaume},
doi = {10.1186/1748-7188-8-22},
file = {:home/morispi1/Cours/PhD/Articles/Assemblage/Minia (2012).pdf:pdf},
journal = {Algorithms for Molecular Biology},
keywords = {bloom filter,de bruijn graph,de novo assembly},
pages = {1--9},
pmid = {24040893},
title = {{Space-e cient and exact de Bruijn graph representation based on a Bloom filter}},
volume = {2},
year = {2013}
}
@article{Boetzer2014,
abstract = {BACKGROUND: The recent introduction of the Pacific Biosciences RS single molecule sequencing technology has opened new doors to scaffolding genome assemblies in a cost-effective manner. The long read sequence information is promised to enhance the quality of incomplete and inaccurate draft assemblies constructed from Next Generation Sequencing (NGS) data.$\backslash$n$\backslash$nRESULTS: Here we propose a novel hybrid assembly methodology that aims to scaffold pre-assembled contigs in an iterative manner using PacBio RS long read information as a backbone. On a test set comprising six bacterial draft genomes, assembled using either a single Illumina MiSeq or Roche 454 library, we show that even a 50× coverage of uncorrected PacBio RS long reads is sufficient to drastically reduce the number of contigs. Comparisons to the AHA scaffolder indicate our strategy is better capable of producing (nearly) complete bacterial genomes.$\backslash$n$\backslash$nCONCLUSIONS: The current work describes our SSPACE-LongRead software which is designed to upgrade incomplete draft genomes using single molecule sequences. We conclude that the recent advances of the PacBio sequencing technology and chemistry, in combination with the limited computational resources required to run our program, allow to scaffold genomes in a fast and reliable manner.},
author = {Boetzer, Marten and Pirovano, Walter},
doi = {10.1186/1471-2105-15-211},
file = {:home/morispi1/Cours/PhD/Articles/Assemblage/SSPACE-Longread.pdf:pdf},
isbn = {1471-2105},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {De novo assembly,Scaffolding,Single molecule seque,de novo assembly,genome finishing,pacific biosciences,scaffolding,single molecule sequencing},
number = {1},
pages = {211},
pmid = {24950923},
title = {{SSPACE-LongRead: scaffolding bacterial draft genomes using long read sequence information.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4076250{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {15},
year = {2014}
}
@article{Biodiversity2016,
author = {Biodiversity, N T H E and Distribution, T H E and Freshwater, O F and Of, Fish and Update, A N Annotated and De, E A U Douce and Annotee, Mise A Jour},
file = {:home/morispi1/Cours/PhD/Articles/Divers/rapport{\_}Genoscope{\_}FXBABIN.pdf:pdf},
journal = {Seven},
pages = {5--9},
title = {{L ' n :}},
volume = {6},
year = {2016}
}
@article{Kielbasa2011,
author = {Kielbasa, Szymon M and Wan, Raymond and Sato, Kengo and Kiebasa, Szymon M and Horton, Paul and Frith, Martin C},
doi = {10.1101/gr.113985.110},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kielbasa et al. - 2011 - Adaptive seeds tame genomic sequence comparison.pdf:pdf},
journal = {Genome Research},
number = {3},
pages = {487--493},
title = {{Adaptive seeds tame genomic sequence comparison}},
volume = {21},
year = {2011}
}
@article{Madoui2015,
abstract = {$\backslash$nMohammed-Amin Madoui and Stefan Engelen contributed equally to this work.$\backslash$nBACKGROUND:Long-read sequencing technologies were launched a few years ago, and in contrast with short-read sequencing technologies, they offered a promise of solving assembly problems for large and complex genomes. Moreover by providing long-range information, it could also solve haplotype phasing. However, existing long-read technologies still have several limitations that complicate their use for most research laboratories, as well as in large and/or complex genome projects. In 2014, Oxford Nanopore released the MinION(R) device, a small and low-cost single-molecule nanopore sequencer, which offers the possibility of sequencing long DNA fragments.RESULTS:The assembly of long reads generated using the Oxford Nanopore MinION(R) instrument is challenging as existing assemblers were not implemented to deal with long reads exhibiting close to 30{\%} of errors. Here, we presented a hybrid approach developed to take advantage of data generated using MinION(R) device. We sequenced a well-known bacterium, Acinetobacter baylyi ADP1 and applied our method to obtain a highly contiguous (one single contig) and accurate genome assembly even in repetitive regions, in contrast to an Illumina-only assembly. Our hybrid strategy was able to generate NaS (Nanopore Synthetic-long) reads up to 60kb that aligned entirely and with no error to the reference genome and that spanned highly conserved repetitive regions. The average accuracy of NaS reads reached 99.99{\%} without losing the initial size of the input MinION(R) reads.CONCLUSIONS:We described NaS tool, a hybrid approach allowing the sequencing of microbial genomes using the MinION(R) device. Our method, based ideally on 20x and 50x of NaS and Illumina reads respectively, provides an efficient and cost-effective way of sequencing microbial or small eukaryotic genomes in a very short time even in small facilities. Moreover, we demonstrated that although the Oxford Nanopore technology is a relatively new sequencing technology, currently with a high error rate, it is already useful in the generation of high-quality genome assemblies.},
author = {Madoui, Mohammed-Amin and Engelen, Stefan and Cruaud, Corinne and Belser, Caroline and Bertrand, Laurie and Alberti, Adriana and Lemainque, Arnaud and Wincker, Patrick and Aury, Jean-Marc},
doi = {10.1186/s12864-015-1519-z},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Madoui et al. - 2015 - Genome assembly using Nanopore-guided long and error-free DNA reads.pdf:pdf},
isbn = {1471-2164 (Electronic)$\backslash$r1471-2164 (Linking)},
issn = {1471-2164},
journal = {BMC Genomics},
keywords = {Nanopore sequencing,Oxford nanopore,MinION{\textregistered} device,de novo genome assembly,device,genome finishing,minion,nanopore sequencing,oxford nanopore},
pages = {327},
pmid = {25927464},
publisher = {???},
title = {{Genome assembly using Nanopore-guided long and error-free DNA reads}},
url = {http://www.biomedcentral.com/1471-2164/16/327},
volume = {16},
year = {2015}
}
@article{Niko20131,
author = {Niko, V},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Niko - 2013 - Scalable and Versatile k -mer Indexing for High-Throughput Sequencing Data.pdf:pdf},
number = {250345},
pages = {237--248},
title = {{Scalable and Versatile k -mer Indexing for High-Throughput Sequencing Data}},
year = {2013}
}
@article{English2012,
abstract = {Many genomes have been sequenced to high-quality draft status using Sanger capillary electrophoresis and/or newer short-read sequence data and whole genome assembly techniques. However, even the best draft genomes contain gaps and other imperfections due to limitations in the input data and the techniques used to build draft assemblies. Sequencing biases, repetitive genomic features, genomic polymorphism, and other complicating factors all come together to make some regions difficult or impossible to assemble. Traditionally, draft genomes were upgraded to "phase 3 finished" status using time-consuming and expensive Sanger-based manual finishing processes. For more facile assembly and automated finishing of draft genomes, we present here an automated approach to finishing using long-reads from the Pacific Biosciences RS (PacBio) platform. Our algorithm and associated software tool, PBJelly, (publicly available at https://sourceforge.net/projects/pb-jelly/) automates the finishing process using long sequence reads in a reference-guided assembly process. PBJelly also provides "lift-over" co-ordinate tables to easily port existing annotations to the upgraded assembly. Using PBJelly and long PacBio reads, we upgraded the draft genome sequences of a simulated Drosophila melanogaster, the version 2 draft Drosophila pseudoobscura, an assembly of the Assemblathon 2.0 budgerigar dataset, and a preliminary assembly of the Sooty mangabey. With 24× mapped coverage of PacBio long-reads, we addressed 99{\%} of gaps and were able to close 69{\%} and improve 12{\%} of all gaps in D. pseudoobscura. With 4× mapped coverage of PacBio long-reads we saw reads address 63{\%} of gaps in our budgerigar assembly, of which 32{\%} were closed and 63{\%} improved. With 6.8× mapped coverage of mangabey PacBio long-reads we addressed 97{\%} of gaps and closed 66{\%} of addressed gaps and improved 19{\%}. The accuracy of gap closure was validated by comparison to Sanger sequencing on gaps from the original D. pseudoobscura draft assembly and shown to be dependent on initial reference quality.},
author = {English, Adam C. and Richards, Stephen and Han, Yi and Wang, Min and Vee, Vanesa and Qu, Jiaxin and Qin, Xiang and Muzny, Donna M. and Reid, Jeffrey G. and Worley, Kim C. and Gibbs, Richard A.},
doi = {10.1371/journal.pone.0047768},
file = {:home/morispi1/Cours/PhD/Articles/Assemblage/PBJelly2.pdf:pdf},
isbn = {1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
number = {11},
pages = {1--12},
pmid = {23185243},
title = {{Mind the Gap: Upgrading Genomes with Pacific Biosciences RS Long-Read Sequencing Technology}},
volume = {7},
year = {2012}
}
@article{Li2008,
abstract = {SUMMARY: We have developed a program SOAP for efficient gapped and ungapped alignment of short oligonucleotides onto reference sequences. The program is designed to handle the huge amounts of short reads generated by parallel sequencing using the new generation Illumina-Solexa sequencing technology. SOAP is compatible with numerous applications, including single-read or pair-end resequencing, small RNA discovery and mRNA tag sequence mapping. SOAP is a command-driven program, which supports multi-threaded parallel computing, and has a batch module for multiple query sets. AVAILABILITY: http://soap.genomics.org.cn.},
author = {Li, Ruiqiang and Li, Yingrui and Kristiansen, Karsten and Wang, Jun},
doi = {10.1093/bioinformatics/btn025},
file = {:home/morispi1/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2008 - SOAP Short oligonucleotide alignment program.pdf:pdf},
isbn = {1367-4811 (Electronic) 1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {5},
pages = {713--714},
pmid = {18227114},
title = {{SOAP: Short oligonucleotide alignment program}},
volume = {24},
year = {2008}
}
